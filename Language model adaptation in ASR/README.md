1G, 2G, 3G are n-gram models with linear interpolation
1G_LOG, 2G_LOG, 3G_LOG are n-gram models with log-linear mixing

These scripts provide optimal lamda values on the basis of iterations and perplexity value

For more information : http://www.speech.sri.com/projects/srilm/

The Neural Network Documentation and Base Code : http://www.fit.vutbr.cz/~imikolov/rnnlm/ OR rnnlm.org

Dataset is GIGAWORD corpus (background) and the in-domain model is WSJ (Wall Street Journal) Dataset -- Both can be found online
